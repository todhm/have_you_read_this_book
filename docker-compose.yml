version: '3.6'
services:

  webapp:
    container_name: webapp
    image: todhm/flask_spark_webapp
    build:
      context: ./flask_with_spark
    working_dir: /app
    command: gunicorn -b 0.0.0.0:8000 --reload -w 4  wsgi:app
    networks:
        - sparknetwork
    ports:
      - "8000:8000"
    volumes:
        - ./flask_with_spark:/app
    depends_on:
      - spark-master
      - redis
      - mongo
    environment:
     - SPARK_APPLICATION_PYTHON_LOCATION =/app/wsgi.py
     - SPARK_MASTER_NAME=spark-master
     - SPARK_MASTER_PORT=7077
     - PYSPARK_PYTHON=python3
     - APP_SETTINGS=settings.DevelopmentConfig

  celery_worker:
    container_name: celery_worker
    image: todhm/flask_spark_webapp
    working_dir: /app
    command: celery -A celery_worker:celery worker  --loglevel=INFO
    networks:
        - sparknetwork
    ports:
      - "9000:9000"
    depends_on:
      - webapp
      - mongo
      - spark-master
      - redis
    environment:
     - "SPARK_MASTER=spark://spark-master:7077"
     - SPARK_APPLICATION_PYTHON_LOCATION =/app/wsgi.py
     - SPARK_MASTER_NAME=spark-master
     - SPARK_MASTER_PORT=7077
     - PYSPARK_PYTHON=python3
     - APP_SETTINGS=settings.DevelopmentConfig

  celery_beat:
    container_name: celery_beat
    image: todhm/flask_spark_webapp
    working_dir: /app
    command: celery -A celery_worker:celery beat  --loglevel=INFO
    networks:
        - sparknetwork
    ports:
      - "9050:9050"
    depends_on:
      - webapp
      - mongo
      - spark-master
      - redis
      - celery_worker
    environment:
     - "SPARK_MASTER=spark://spark-master:7077"
     - SPARK_APPLICATION_PYTHON_LOCATION =/app/wsgi.py
     - SPARK_MASTER_NAME=spark-master
     - SPARK_MASTER_PORT=7077
     - PYSPARK_PYTHON=python3
     - APP_SETTINGS=settings.DevelopmentConfig


  mongo:
    image: mongo:3.4
    restart: always
    volumes:
      - db-data:/data/db
      - ./mongo/sample_shoppings:/docker-entrypoint-initdb.d/sample_shoppings
      - ./mongo/restore.sh:/docker-entrypoint-initdb.d/restore.sh
    networks:
      - sparknetwork

  redis:
    image: redis:alpine
    networks:
        - sparknetwork

  spark-master:
    image: bde2020/spark-master:2.3.0-hadoop2.7
    container_name: spark-master
    networks:
    - sparknetwork
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - "constraint:node==default"

  spark-worker-1:
    image: bde2020/spark-worker:2.3.0-hadoop2.7
    container_name: spark-worker-1
    networks:
    - sparknetwork
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"

  spark-worker-2:
    image: bde2020/spark-worker:2.3.0-hadoop2.7
    container_name: spark-worker-2
    networks:
    - sparknetwork
    depends_on:
      - spark-master
    ports:
      - "8082:8082"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"


volumes:
  db-data:

networks:
    sparknetwork:
